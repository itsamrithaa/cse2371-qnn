{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import re, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer: handles Cl/Br, bracket atoms, and %nn tokens\n",
    "TOKENIZER_RE = re.compile(r\"(\\%\\d{2}|\\[[^\\]]+\\]|Br|Cl|.)\")\n",
    "\n",
    "def tokenize_smiles(smi: str):\n",
    "    return TOKENIZER_RE.findall(smi)\n",
    "\n",
    "def build_vocab(smiles_list, min_freq=1):\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for s in smiles_list:\n",
    "        cnt.update(tokenize_smiles(s))\n",
    "    itos = [\"<PAD>\", \"<UNK>\"]\n",
    "    for tok, f in cnt.most_common():\n",
    "        if f >= min_freq and tok not in itos:\n",
    "            itos.append(tok)\n",
    "    stoi = {t:i for i,t in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "def encode_smiles(smi, stoi, max_len):\n",
    "    ids = [stoi.get(t, stoi[\"<UNK>\"]) for t in tokenize_smiles(smi)[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [stoi[\"<PAD>\"]] * (max_len - len(ids))\n",
    "    return np.array(ids, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "\n",
    "\n",
    "class SmilesCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, max_len=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(emb_dim, 128, kernel_size=3, padding=1),\n",
    "            nn.Conv1d(emb_dim, 128, kernel_size=5, padding=2),\n",
    "            nn.Conv1d(emb_dim, 128, kernel_size=7, padding=3),\n",
    "        ])\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128*3, 256), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, x):          # x: (B, L)\n",
    "        e = self.emb(x)            # (B, L, E)\n",
    "        e = e.transpose(1, 2)      # (B, E, L)\n",
    "        feats = [torch.amax(self.act(conv(e)), dim=-1) for conv in self.convs]  # (B,128) x3\n",
    "        h = torch.cat(feats, dim=1)\n",
    "        h = self.dropout(h)\n",
    "        y = self.head(h).squeeze(1)  # (B,)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and evaluation \n",
    "\n",
    "\n",
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles, y, stoi, max_len):\n",
    "        self.X = [encode_smiles(s, stoi, max_len) for s in smiles]\n",
    "        self.y = None if y is None else np.array(y, dtype=np.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        ids = torch.from_numpy(self.X[i])\n",
    "        if self.y is None: return ids\n",
    "        return ids, torch.tensor(self.y[i])\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).cpu().numpy()\n",
    "        ys.append(yb.numpy()); ps.append(pred)\n",
    "    y = np.concatenate(ys); p = np.concatenate(ps)\n",
    "    rmse = float(np.sqrt(((y - p) ** 2).mean()))   # manual RMSE\n",
    "    mae  = float(np.mean(np.abs(y - p)))           # manual MAE\n",
    "    r2   = r2_score(y, p)                          # OK to keep this\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:05] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples after cleaning: 6110\n",
      "Vocab size: 187\n",
      "Epoch 001 | train_loss 4.9289 | val_RMSE 1.8231 MAE 1.3443 R2 0.483\n",
      "Epoch 002 | train_loss 2.9659 | val_RMSE 1.5605 MAE 1.1748 R2 0.621\n",
      "Epoch 003 | train_loss 2.5074 | val_RMSE 1.5401 MAE 1.1256 R2 0.631\n",
      "Epoch 004 | train_loss 2.2254 | val_RMSE 1.4087 MAE 1.0613 R2 0.691\n",
      "Epoch 005 | train_loss 2.0576 | val_RMSE 1.4753 MAE 1.1051 R2 0.662\n",
      "Epoch 006 | train_loss 1.9042 | val_RMSE 1.4064 MAE 1.0357 R2 0.692\n",
      "Epoch 007 | train_loss 1.7490 | val_RMSE 1.3337 MAE 0.9579 R2 0.723\n",
      "Epoch 008 | train_loss 1.6761 | val_RMSE 1.3415 MAE 0.9540 R2 0.720\n",
      "Epoch 009 | train_loss 1.6966 | val_RMSE 1.3492 MAE 0.9645 R2 0.717\n",
      "Epoch 010 | train_loss 1.5835 | val_RMSE 1.3815 MAE 0.9865 R2 0.703\n",
      "Epoch 011 | train_loss 1.5318 | val_RMSE 1.4271 MAE 1.0360 R2 0.683\n",
      "Epoch 012 | train_loss 1.6830 | val_RMSE 1.3157 MAE 0.9516 R2 0.731\n",
      "Epoch 013 | train_loss 1.3974 | val_RMSE 1.2922 MAE 0.9310 R2 0.740\n",
      "Epoch 014 | train_loss 1.3697 | val_RMSE 1.3260 MAE 0.9578 R2 0.727\n",
      "Epoch 015 | train_loss 1.4068 | val_RMSE 1.3270 MAE 0.9525 R2 0.726\n",
      "Epoch 016 | train_loss 1.5165 | val_RMSE 1.3152 MAE 0.9453 R2 0.731\n",
      "Epoch 017 | train_loss 1.2883 | val_RMSE 1.2856 MAE 0.8991 R2 0.743\n",
      "Epoch 018 | train_loss 1.2904 | val_RMSE 1.2874 MAE 0.9141 R2 0.742\n",
      "Epoch 019 | train_loss 1.2710 | val_RMSE 1.4043 MAE 1.0076 R2 0.693\n",
      "Epoch 020 | train_loss 1.2465 | val_RMSE 1.3195 MAE 0.9323 R2 0.729\n",
      "Epoch 021 | train_loss 1.2048 | val_RMSE 1.3247 MAE 0.9459 R2 0.727\n",
      "Epoch 022 | train_loss 1.1786 | val_RMSE 1.3436 MAE 0.9679 R2 0.719\n",
      "Epoch 023 | train_loss 1.1358 | val_RMSE 1.2743 MAE 0.8894 R2 0.748\n",
      "Epoch 024 | train_loss 1.1686 | val_RMSE 1.3493 MAE 0.9726 R2 0.717\n",
      "Epoch 025 | train_loss 1.2453 | val_RMSE 1.2407 MAE 0.8769 R2 0.761\n",
      "Epoch 026 | train_loss 1.1809 | val_RMSE 1.3856 MAE 1.0121 R2 0.702\n",
      "Epoch 027 | train_loss 1.1969 | val_RMSE 1.2891 MAE 0.8901 R2 0.742\n",
      "Epoch 028 | train_loss 1.1502 | val_RMSE 1.2344 MAE 0.8561 R2 0.763\n",
      "Epoch 029 | train_loss 1.1554 | val_RMSE 1.4513 MAE 1.0697 R2 0.673\n",
      "Epoch 030 | train_loss 1.1351 | val_RMSE 1.3912 MAE 1.0038 R2 0.699\n",
      "Saved best weights to best_cnn.pt\n",
      "\n",
      "Final metrics\n",
      "Train: RMSE 0.9706 | MAE 0.7116 | R2 0.860\n",
      "Val:   RMSE 1.3912 | MAE 1.0038 | R2 0.699\n",
      "Test:  RMSE 1.4360 | MAE 1.0333 | R2 0.697\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# ---- Config ----\n",
    "CSV_PATH = \"/Users/amrithaa/Downloads/AqSolDB-master/data/dataset-A.csv\"       \n",
    "SMILES_COL = \"SMILES\"\n",
    "TARGET_COL = \"Solubility\"\n",
    "MAX_LEN = 256\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "LR = 2e-3\n",
    "DROPOUT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- Load data ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert SMILES_COL in df.columns and TARGET_COL in df.columns, f\"Missing columns. Got: {list(df.columns)}\"\n",
    "\n",
    "smiles, y = [], []\n",
    "for s, t in zip(df[SMILES_COL].astype(str), df[TARGET_COL]):\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if not m: \n",
    "        continue\n",
    "    smiles.append(Chem.MolToSmiles(m))\n",
    "    try:\n",
    "        y.append(float(t))\n",
    "    except:\n",
    "        y.append(np.nan)\n",
    "\n",
    "smiles = np.array(smiles)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "mask = np.isfinite(y)\n",
    "smiles, y = smiles[mask], y[mask]\n",
    "print(f\"Samples after cleaning: {len(smiles)}\")\n",
    "\n",
    "# ---- Split ---- (simple random 80/10/10 for a quick start)\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(smiles, y, test_size=0.2, random_state=SEED)\n",
    "X_val, X_test, y_val, y_test   = train_test_split(X_tmp, y_tmp,  test_size=0.5, random_state=SEED)\n",
    "\n",
    "# ---- Vocab ----\n",
    "stoi, itos = build_vocab(X_train.tolist())\n",
    "with open(\"vocab.json\", \"w\") as f: json.dump(itos, f)\n",
    "print(\"Vocab size:\", len(itos))\n",
    "\n",
    "# ---- DataLoaders ----\n",
    "train_ds = SmilesDataset(X_train, y_train, stoi, MAX_LEN)\n",
    "val_ds   = SmilesDataset(X_val,   y_val,   stoi, MAX_LEN)\n",
    "test_ds  = SmilesDataset(X_test,  y_test,  stoi, MAX_LEN)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_ld   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ---- Model/opt ----\n",
    "model = SmilesCNN(vocab_size=len(itos), emb_dim=128, max_len=MAX_LEN, dropout=DROPOUT).to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "crit  = nn.MSELoss()\n",
    "\n",
    "best_rmse, best_state, patience, waited = float(\"inf\"), None, 8, 0\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for xb, yb in train_ld:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = crit(pred, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    rmse, mae, r2 = evaluate(model, val_ld, device)\n",
    "    print(f\"Epoch {ep:03d} | train_loss {np.mean(losses):.4f} | val_RMSE {rmse:.4f} MAE {mae:.4f} R2 {r2:.3f}\")\n",
    "\n",
    "    if rmse < best_rmse - 1e-4:\n",
    "        best_rmse, best_state, waited = rmse, {k:v.cpu() for k,v in model.state_dict().items()}, 0\n",
    "    else:\n",
    "        waited += 1\n",
    "        if waited >= patience:\n",
    "            print(\"Early stopping.\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict({k: v.to(device) for k,v in best_state.items()})\n",
    "    torch.save(best_state, \"best_cnn.pt\")\n",
    "    print(\"Saved best weights to best_cnn.pt\")\n",
    "\n",
    "# ---- Final metrics ----\n",
    "tr_rmse, tr_mae, tr_r2 = evaluate(model, train_ld, device)\n",
    "va_rmse, va_mae, va_r2 = evaluate(model, val_ld, device)\n",
    "te_rmse, te_mae, te_r2 = evaluate(model, test_ld, device)\n",
    "\n",
    "print(\"\\nFinal metrics\")\n",
    "print(f\"Train: RMSE {tr_rmse:.4f} | MAE {tr_mae:.4f} | R2 {tr_r2:.3f}\")\n",
    "print(f\"Val:   RMSE {va_rmse:.4f} | MAE {va_mae:.4f} | R2 {va_r2:.3f}\")\n",
    "print(f\"Test:  RMSE {te_rmse:.4f} | MAE {te_mae:.4f} | R2 {te_r2:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CSE 2371 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
